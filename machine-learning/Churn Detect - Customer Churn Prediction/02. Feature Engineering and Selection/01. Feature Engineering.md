# Module 2: Feature Engineering and Selection

## Task: Perform Feature Engineering for Churn Prediction

In this task, we will focus on feature engineering, which involves creating new features or transforming existing ones to enhance the predictive power of our model for customer churn prediction. Feature engineering helps in capturing relevant information and patterns in the data that can assist in identifying potential churners. We will explore various techniques to derive meaningful features from the existing dataset.

### Step 1: Load the Data

The first step is to load the preprocessed and cleaned dataset that was prepared in the previous module. Make sure you have the dataset file available and accessible.

```python
import pandas as pd

# Load the preprocessed dataset
data = pd.read_csv('preprocessed_data.csv')

# Display the initial few rows of the dataset
data.head()
```

### Step 2: Explore Existing Features

Before diving into feature engineering, let's take a closer look at the existing features and their distributions to gain a better understanding of the data.

```python
# Display summary statistics of the dataset
data.describe()

# Explore the distribution of categorical variables
categorical_features = ['gender', 'age_group', 'region']
for feature in categorical_features:
    print(data[feature].value_counts())

# Explore the distribution of numerical variables
numerical_features = ['total_purchase_amount', 'avg_session_duration']
for feature in numerical_features:
    data[feature].hist()
```

By analyzing the summary statistics and distributions of features, you can identify potential outliers, skewed distributions, or interesting patterns that may require further investigation or transformation during feature engineering.

### Step 3: Create New Features

Feature engineering involves creating new features or deriving insights from existing features that could be valuable for predicting customer churn. Let's explore a few techniques to generate new features:

#### Example 1: Recency of Last Interaction

One important factor in customer churn is the recency of their last interaction with the platform. By calculating the number of days between the current date and the customer's last interaction date, we can capture this information as a new feature.

```python
import datetime

# Convert the date column to datetime type
data['last_interaction_date'] = pd.to_datetime(data['last_interaction_date'])

# Calculate recency of last interaction
current_date = datetime.datetime.now()
data['recency'] = (current_date - data['last_interaction_date']).dt.days

# Verify the new feature
data[['last_interaction_date', 'recency']].head()
```

#### Example 2: Aggregated Metrics

We can also aggregate certain metrics to capture the overall behavior of customers. For instance, we can calculate the total number of purchases made by each customer or the average session duration across multiple interactions.

```python
# Aggregate total purchase amount by customer
total_purchase_by_customer = data.groupby('customer_id')['total_purchase_amount'].sum().reset_index()
total_purchase_by_customer.rename(columns={'total_purchase_amount': 'total_purchase'}, inplace=True)

# Aggregate average session duration by customer
avg_session_duration_by_customer = data.groupby('customer_id')['avg_session_duration'].mean().reset_index()
avg_session_duration_by_customer.rename(columns={'avg_session_duration': 'average_session_duration'}, inplace=True)

# Merge the new features with the original dataset
data = pd.merge(data, total_purchase_by_customer, on='customer_id')
data = pd.merge(data, avg_session_duration_by_customer, on='customer_id')

# Verify the new features
data[['customer_id', 'total_purchase', 'average_session_duration']].head()
```

### Step 4: Transform Existing Features

In addition to creating new features, we can transform existing features to uncover hidden patterns or make them more suitable for modeling. Let's consider a few transformation techniques:

#### Example 

1: Log Transformation

If numerical features exhibit skewed distributions, applying a log transformation can help normalize the data and reduce the impact of outliers.

```python
import numpy as np

# Apply log transformation to 'total_purchase_amount'
data['log_purchase_amount'] = np.log1p(data['total_purchase_amount'])

# Verify the transformed feature
data[['total_purchase_amount', 'log_purchase_amount']].head()
```

#### Example 2: One-Hot Encoding

Categorical variables often require conversion into numerical format for modeling purposes. One-hot encoding is a popular technique that creates binary columns for each category, indicating the presence or absence of that category for each data point.

```python
# Perform one-hot encoding for the 'region' feature
one_hot_encoded = pd.get_dummies(data['region'], prefix='region')

# Concatenate the one-hot encoded columns with the original dataset
data = pd.concat([data, one_hot_encoded], axis=1)

# Verify the one-hot encoded columns
one_hot_encoded.head()
```

### Step 5: Feature Selection

After performing feature engineering, it is essential to select the most relevant features that contribute significantly to predicting customer churn. This helps in reducing dimensionality and eliminating noise or irrelevant information. Let's explore a couple of techniques for feature selection:

#### Example 1: Correlation Analysis

Correlation analysis measures the relationship between features and the target variable. We can calculate the correlation coefficients and select features with high correlation values.

```python
# Calculate correlation matrix
correlation_matrix = data.corr()

# Identify features highly correlated with the target variable ('churn')
highly_correlated_features = correlation_matrix['churn'][abs(correlation_matrix['churn']) > 0.1]

# Select the highly correlated features
selected_features = highly_correlated_features.index.tolist()
selected_features.remove('churn')

# Display the selected features
selected_features
```

#### Example 2: Feature Importance from Tree-Based Models

Tree-based models, such as Random Forest or XGBoost, provide feature importance scores. We can leverage these scores to select the most relevant features for churn prediction.

```python
from sklearn.ensemble import RandomForestClassifier

# Prepare the feature matrix and target variable
X = data.drop(columns=['churn'])
y = data['churn']

# Train a Random Forest classifier
rf_model = RandomForestClassifier()
rf_model.fit(X, y)

# Get feature importances
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)

# Select features with importance above a certain threshold
selected_features = feature_importances[feature_importances > 0.01].index.tolist()

# Display the selected features
selected_features
```

