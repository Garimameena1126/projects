# Task: Investigate Ensemble Methods to Improve Model Accuracy and Stability

In this task, we will explore ensemble methods such as Voting, Bagging, and Boosting to enhance the performance, accuracy, and stability of our churn prediction model. Ensemble methods combine multiple individual models to make predictions collectively, often resulting in better overall performance than using a single model. We will cover the following steps:

1. Introduction to Ensemble Methods
2. Voting Ensemble
3. Bagging Ensemble
4. Boosting Ensemble

## 1. Introduction to Ensemble Methods

Ensemble methods combine the predictions of multiple models to create a more robust and accurate final prediction. They leverage the principle of "wisdom of the crowd," where diverse models collectively yield better results than any individual model. There are three primary types of ensemble methods: Voting, Bagging, and Boosting.

- Voting: Combines the predictions from multiple models by majority voting or weighted voting.
- Bagging: Trains multiple models on different subsets of the training data and averages their predictions.
- Boosting: Trains multiple models sequentially, with each subsequent model focusing on the mistakes made by the previous models.

Now that we understand the basics of ensemble methods, let's explore each type in more detail.

## 2. Voting Ensemble

A voting ensemble combines the predictions from multiple individual models to make a final prediction. There are two main types of voting ensembles:

- Hard Voting: In hard voting, each model in the ensemble predicts the class label, and the class with the majority of votes is selected as the final prediction.
- Soft Voting: In soft voting, each model predicts the class probabilities, and the average probabilities are calculated across all models. The class with the highest average probability is chosen as the final prediction.

Here's how you can implement a voting ensemble using scikit-learn in Python:

```python
from sklearn.ensemble import VotingClassifier

# Initialize individual classifiers
classifier1 = LogisticRegression()
classifier2 = RandomForestClassifier()
classifier3 = XGBClassifier()

# Create a voting ensemble
voting_classifier = VotingClassifier(
    estimators=[('lr', classifier1), ('rf', classifier2), ('xgb', classifier3)],
    voting='hard'  # or 'soft' for soft voting
)

# Train the voting ensemble
voting_classifier.fit(X_train, y_train)

# Evaluate the ensemble's performance
accuracy = voting_classifier.score(X_test, y_test)
```

In the code above, we first initialize individual classifiers such as LogisticRegression, RandomForestClassifier, and XGBClassifier. We then create a VotingClassifier, specifying the individual classifiers and the voting type (hard or soft). Finally, we fit the ensemble on the training data and evaluate its performance using a suitable evaluation metric.

## 3. Bagging Ensemble

Bagging, short for Bootstrap Aggregating, is an ensemble method that trains multiple models on different subsets of the training data and combines their predictions through averaging (for regression) or majority voting (for classification). Random Forest is a popular implementation of bagging.

Here's how you can implement a bagging ensemble using scikit-learn:

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# Initialize a base classifier
base_classifier = DecisionTreeClassifier()

# Create a bagging ensemble
bagging_classifier = BaggingClassifier(
    base_estimator=base_classifier,
    n_estimators=10  # Number of base classifiers
)

# Train the bagging ensemble
bagging_classifier.fit(X_train, y_train)

# Evaluate the ensemble's performance
accuracy = bagging_classifier.score(X_test, y_test)
```

In the code above, we initialize a base classifier (e.g., DecisionTreeClassifier) and create a BaggingClassifier.

 We specify the base classifier and the number of base classifiers (n_estimators) to include in the ensemble. Finally, we fit the ensemble on the training data and evaluate its performance.

## 4. Boosting Ensemble

Boosting is an ensemble method that trains multiple models sequentially, with each subsequent model focusing on the mistakes made by the previous models. The idea is to build a strong model by iteratively correcting the weaknesses of individual models. AdaBoost and Gradient Boosting are popular implementations of boosting.

Here's an example of implementing a boosting ensemble using the XGBoost library in Python:

```python
import xgboost as xgb

# Define the boosting parameters
boosting_params = {
    'objective': 'binary:logistic',
    'n_estimators': 100,  # Number of boosting rounds
    'learning_rate': 0.1,
    'max_depth': 3
}

# Create a boosting ensemble
boosting_classifier = xgb.XGBClassifier(**boosting_params)

# Train the boosting ensemble
boosting_classifier.fit(X_train, y_train)

# Evaluate the ensemble's performance
accuracy = boosting_classifier.score(X_test, y_test)
```

In the code above, we define the boosting parameters, including the boosting objective, number of boosting rounds (n_estimators), learning rate, and maximum depth of the individual trees. We then create an XGBClassifier with the specified parameters and fit it on the training data. Finally, we evaluate the ensemble's performance using a suitable evaluation metric.

