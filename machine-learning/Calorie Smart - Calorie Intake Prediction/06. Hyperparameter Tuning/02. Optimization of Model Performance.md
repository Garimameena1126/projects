# Hyperparameter Tuning using Grid Search and Random Search

In this guide, we will explore the techniques of grid search and random search to find the optimal combination of hyperparameters for our chosen regression model. Hyperparameter tuning is an essential step in machine learning to improve the model's performance and achieve better results. We will focus on two common approaches: grid search and random search. 

## Step 1: Define Hyperparameters

Before we dive into hyperparameter tuning, let's define the hyperparameters we want to optimize. In our regression model, some common hyperparameters include learning rate, regularization strength, number of hidden layers, and number of neurons in each layer. 

For this guide, let's assume we are using a neural network as our regression model, and we want to tune the following hyperparameters:

1. Learning rate (lr): The step size at each iteration during training.
2. Regularization strength (reg): Controls the amount of regularization applied to the model.
3. Number of hidden layers (layers): The number of hidden layers in the neural network.
4. Number of neurons per layer (neurons): The number of neurons in each hidden layer.

## Step 2: Prepare the Data

Before we proceed with hyperparameter tuning, ensure that you have preprocessed and split your data into training and testing sets. This step is crucial for unbiased evaluation of different hyperparameter combinations.

Let's assume you have already completed the data preprocessing and split your data into `X_train`, `X_test`, `y_train`, and `y_test`, where `X` represents the feature matrix and `y` represents the target variable.

## Step 3: Grid Search

Grid search involves creating a grid of possible hyperparameter values and exhaustively searching through all combinations. We will use scikit-learn's `GridSearchCV` class to perform grid search. Follow the steps below to implement grid search:

1. Import the necessary libraries:
```python
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPRegressor
```

2. Create an instance of the regression model:
```python
model = MLPRegressor()
```

3. Define the hyperparameter grid:
```python
param_grid = {
    'learning_rate_init': [0.001, 0.01, 0.1],
    'alpha': [0.0001, 0.001, 0.01],
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],
}
```

4. Create a `GridSearchCV` object with the model and hyperparameter grid:
```python
grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)
```

5. Fit the grid search object to the training data:
```python
grid_search.fit(X_train, y_train)
```

6. Retrieve the best hyperparameters and the corresponding model:
```python
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
```

7. Evaluate the best model on the test data:
```python
best_model_score = best_model.score(X_test, y_test)
```

## Step 4: Random Search

Random search involves randomly sampling hyperparameter combinations from a defined search space. We will use scikit-learn's `RandomizedSearchCV` class to perform random search. Follow the steps below to implement random search:

1. Import the necessary libraries:
```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neural_network import MLPRegressor
from scipy.stats import uniform, randint
```

2. Create an instance of the regression model:
```python
model = MLPRegressor()
```

3. Define the hyper

parameter distributions:
```python
param_dist = {
    'learning_rate_init': uniform(0.001, 0.1),
    'alpha': uniform(0.0001, 0.01),
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],
    'activation': ['relu', 'tanh'],
}
```

4. Create a `RandomizedSearchCV` object with the model and hyperparameter distributions:
```python
random_search = RandomizedSearchCV(model, param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)
```

5. Fit the random search object to the training data:
```python
random_search.fit(X_train, y_train)
```

6. Retrieve the best hyperparameters and the corresponding model:
```python
best_params = random_search.best_params_
best_model = random_search.best_estimator_
```

7. Evaluate the best model on the test data:
```python
best_model_score = best_model.score(X_test, y_test)
```

