# Module 7: Handling Imbalanced Data

## Task: Assess the impact of data balancing on the model's performance and provide an evaluation report.

In this task, we will explore strategies to handle imbalanced sentiment classes in our dataset. Imbalanced data refers to a situation where the classes in the dataset are not represented equally. This can often lead to biased models that perform poorly on minority classes. We will address this issue by applying oversampling and undersampling techniques to balance the sentiment labels. Finally, we will evaluate the impact of data balancing on the performance of our sentiment analysis model and provide an evaluation report.

## Step 1: Load the Dataset

Before we begin, make sure you have a labeled dataset of customer support messages along with their sentiment labels. You can either use an existing dataset or create your own. Ensure that the dataset contains a sufficient number of samples for each sentiment class.

Let's assume you have a CSV file named `customer_support_data.csv` containing two columns: `message` (the customer support message) and `sentiment` (the corresponding sentiment label).

To load the dataset, we can use the pandas library in Python:

```python
import pandas as pd

# Load the dataset from CSV
dataset = pd.read_csv('customer_support_data.csv')
```

## Step 2: Explore the Dataset

Now that we have loaded the dataset, let's explore it to understand the distribution of sentiment labels. This will help us identify the class imbalance issue.

```python
# Count the number of samples in each sentiment class
sentiment_counts = dataset['sentiment'].value_counts()

# Calculate the percentage of samples in each class
sentiment_percentages = sentiment_counts / len(dataset) * 100

# Display the class distribution
print(sentiment_counts)
print(sentiment_percentages)
```

By examining the class distribution, you can determine if there is a significant imbalance between the sentiment classes. If one or more classes have a much smaller representation compared to others, we need to address this issue to improve the model's performance on minority classes.

## Step 3: Split the Dataset

Before applying data balancing techniques, it's important to split the dataset into training and testing sets. This will allow us to evaluate the impact of data balancing on the model's performance using unseen data.

We can use the `train_test_split` function from scikit-learn to split the dataset into training and testing sets:

```python
from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
train_data, test_data, train_labels, test_labels = train_test_split(
    dataset['message'], dataset['sentiment'], test_size=0.2, random_state=42
)
```

Now we have the training data, training labels, testing data, and testing labels ready for further processing.

## Step 4: Data Balancing Techniques

To address the class imbalance issue, we will explore two common techniques: oversampling and undersampling. Oversampling increases the number of samples in the minority class, while undersampling reduces the number of samples in the majority class.

### Oversampling using SMOTE

SMOTE (Synthetic Minority Over-sampling Technique) is a popular oversampling technique that generates synthetic samples for the minority class based on the characteristics of existing samples.

First, let's install the imbalanced-learn library, which provides various imbalanced data handling techniques including SMOTE:

```
pip install imbalanced-learn
```

Next, we can use SMOTE to oversample the minority class in the training data:

```python
from imblearn.over_sampling import SMOTE

# Apply SMOTE oversampling
smote = SMOTE(random_state=42)
oversampled_train_data, oversampled_train_labels = smote

.fit_resample(train_data, train_labels)
```

Now we have `oversampled_train_data` and `oversampled_train_labels` that contain the oversampled data.

### Undersampling using RandomUnderSampler

RandomUnderSampler is a simple undersampling technique that randomly selects samples from the majority class to match the number of samples in the minority class.

Let's install the imbalanced-learn library if you haven't already:

```
pip install imbalanced-learn
```

We can then use RandomUnderSampler to undersample the majority class in the training data:

```python
from imblearn.under_sampling import RandomUnderSampler

# Apply RandomUnderSampler undersampling
undersampler = RandomUnderSampler(random_state=42)
undersampled_train_data, undersampled_train_labels = undersampler.fit_resample(train_data, train_labels)
```

Now we have `undersampled_train_data` and `undersampled_train_labels` that contain the undersampled data.

## Step 5: Train and Evaluate the Model

After applying data balancing techniques, we can proceed with training a sentiment analysis model on the balanced training data. The choice of the model and its configuration will depend on your specific requirements and preferences.

For example, you can use a Random Forest classifier from scikit-learn as follows:

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Train the model on the balanced training data
rf_classifier.fit(oversampled_train_data, oversampled_train_labels)

# Make predictions on the testing data
predictions = rf_classifier.predict(test_data)

# Evaluate the model's performance
report = classification_report(test_labels, predictions)
print(report)
```

The `classification_report` function calculates various metrics such as precision, recall, F1-score, and accuracy for each sentiment class.

## Step 6: Generate Evaluation Report

Finally, based on the model's performance on the balanced data, we can generate an evaluation report to assess the impact of data balancing on the sentiment analysis model.

You can create a function that takes the original and balanced datasets, trains the model, makes predictions, and generates the evaluation report:

```python
def evaluate_data_balancing(original_data, original_labels, balanced_data, balanced_labels):
    # Train the model on the balanced training data
    rf_classifier.fit(balanced_data, balanced_labels)

    # Make predictions on the original testing data
    original_predictions = rf_classifier.predict(test_data)

    # Evaluate the model's performance on the original testing data
    original_report = classification_report(original_labels, original_predictions)

    # Make predictions on the balanced testing data
    balanced_predictions = rf_classifier.predict(test_data)

    # Evaluate the model's performance on the balanced testing data
    balanced_report = classification_report(test_labels, balanced_predictions)

    # Generate the evaluation report
    evaluation_report = f"Original Data Performance:\n{original_report}\n\nBalanced Data Performance:\n{balanced_report}"
    
    return evaluation_report

# Generate the evaluation report
evaluation_report = evaluate_data_balancing(train_data, train_labels, oversampled_train_data, oversampled_train_labels)
print(evaluation_report)
```

This evaluation report provides insights into how data balancing techniques affect the model's performance on both the original and balanced testing data.
