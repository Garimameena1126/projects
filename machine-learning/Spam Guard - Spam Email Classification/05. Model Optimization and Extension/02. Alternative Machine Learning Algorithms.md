# Investigating Alternative Machine Learning Algorithms

In this guide, we will explore different machine learning algorithms as alternatives to the initial model used for email classification. We will compare their performance and evaluate which algorithm yields the best results. This task is part of Module 5: Model Optimization and Extension. Let's get started!

## Step 1: Selecting Alternative Algorithms
The first step is to choose alternative machine learning algorithms that we want to investigate. Here, we will explore two popular algorithms: Random Forest and Gradient Boosting. These algorithms often perform well in various classification tasks. 

## Step 2: Data Preparation
Before we proceed with training and evaluating the alternative algorithms, we need to ensure that our data is properly prepared. Follow these steps to prepare the data:

1. Load the preprocessed dataset that was split into training and testing sets during the Data Preprocessing and Exploration phase.

2. Separate the features (email content) and the corresponding labels (spam or non-spam) for both the training and testing datasets.

3. Ensure that the feature vectors are in the appropriate format required by the machine learning algorithms. If needed, perform any additional preprocessing specific to the chosen algorithms, such as converting text to numerical representations (e.g., TF-IDF, word embeddings).

## Step 3: Training and Evaluating the Alternative Algorithms
In this step, we will train the alternative machine learning algorithms on the preprocessed training data and evaluate their performance on the testing data. Follow these steps for each algorithm:

### 3.1 Random Forest

Random Forest is an ensemble learning method that combines multiple decision trees. It can handle both numerical and categorical features and is less prone to overfitting. Follow these steps to train and evaluate the Random Forest algorithm:

1. Import the necessary libraries, including scikit-learn.

2. Create an instance of the Random Forest classifier, specifying any desired hyperparameters. You can refer to the scikit-learn documentation for more details on available hyperparameters.

3. Train the Random Forest classifier using the preprocessed training data.

    ```python
    from sklearn.ensemble import RandomForestClassifier
    
    # Create an instance of the Random Forest classifier
    rf_classifier = RandomForestClassifier(n_estimators=100)
    
    # Train the classifier
    rf_classifier.fit(X_train, y_train)
    ```

4. Once the classifier is trained, use it to predict labels for the testing data.

    ```python
    # Predict labels for the testing data
    rf_predictions = rf_classifier.predict(X_test)
    ```

5. Evaluate the performance of the Random Forest classifier using appropriate metrics such as accuracy, precision, recall, and F1-score.

    ```python
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    # Calculate accuracy
    rf_accuracy = accuracy_score(y_test, rf_predictions)
    
    # Calculate precision
    rf_precision = precision_score(y_test, rf_predictions)
    
    # Calculate recall
    rf_recall = recall_score(y_test, rf_predictions)
    
    # Calculate F1-score
    rf_f1_score = f1_score(y_test, rf_predictions)
    ```

### 3.2 Gradient Boosting

Gradient Boosting is another ensemble learning method that combines multiple weak learners (e.g., decision trees) in a sequential manner. It focuses on correcting the mistakes made by the previous learners. Follow these steps to train and evaluate the Gradient Boosting algorithm:

1. Import the necessary libraries, including scikit-learn.

2. Create an instance of the Gradient Boosting classifier, specifying any desired hyperparameters. You can refer to the scikit-learn documentation for more details on available hyperparameters.

3. Train the Gradient Boosting classifier using the preprocessed training data.

    ```python
    from sklearn.ensemble import GradientBoostingClassifier
    
    # Create an instance of the Gradient Boosting classifier
    gb
    
    _classifier = GradientBoostingClassifier(n_estimators=100)
    
    # Train the classifier
    gb_classifier.fit(X_train, y_train)
    ```

4. Once the classifier is trained, use it to predict labels for the testing data.

    ```python
    # Predict labels for the testing data
    gb_predictions = gb_classifier.predict(X_test)
    ```

5. Evaluate the performance of the Gradient Boosting classifier using appropriate metrics such as accuracy, precision, recall, and F1-score.

    ```python
    # Calculate accuracy
    gb_accuracy = accuracy_score(y_test, gb_predictions)
    
    # Calculate precision
    gb_precision = precision_score(y_test, gb_predictions)
    
    # Calculate recall
    gb_recall = recall_score(y_test, gb_predictions)
    
    # Calculate F1-score
    gb_f1_score = f1_score(y_test, gb_predictions)
    ```

## Step 4: Performance Comparison
After evaluating the alternative algorithms, we can compare their performance with the initial model used for email classification. Compare the performance metrics obtained from each algorithm and analyze which algorithm performs better in terms of accuracy, precision, recall, and F1-score.

```python
# Compare performance metrics
print("Random Forest Metrics:")
print("Accuracy:", rf_accuracy)
print("Precision:", rf_precision)
print("Recall:", rf_recall)
print("F1-score:", rf_f1_score)

print("\nGradient Boosting Metrics:")
print("Accuracy:", gb_accuracy)
print("Precision:", gb_precision)
print("Recall:", gb_recall)
print("F1-score:", gb_f1_score)
```

Based on the comparison, you can make an informed decision on whether to switch to one of the alternative algorithms or stick with the initial model.
