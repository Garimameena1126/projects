# Module 3: Model Training and Evaluation

## Task: Evaluate the model's performance using appropriate metrics (accuracy, precision, recall, F1-score, etc.).

Once you have trained your sentiment analysis model using the preprocessed training dataset, it's crucial to evaluate its performance using appropriate metrics. Evaluating the model helps you understand how well it is performing and provides insights into its strengths and weaknesses. In this guide, we will explore various evaluation metrics commonly used for sentiment analysis tasks, such as accuracy, precision, recall, and F1-score.

### Step 1: Load the Preprocessed Test Dataset

Before evaluating the model, you need to load the preprocessed test dataset. This dataset contains customer support messages along with their corresponding sentiment labels. Make sure you have split your original dataset into training and testing sets during the data collection and exploration phase.

Let's assume you have the preprocessed test dataset stored in a CSV file named `test_data.csv`, with two columns: `message` and `sentiment`.

```python
import pandas as pd

# Load the preprocessed test dataset
test_data = pd.read_csv('test_data.csv')

# Display the first few rows of the test dataset
print(test_data.head())
```

The above code snippet reads the test dataset from the CSV file and displays the first few rows using the `head()` function. Make sure to adjust the file path according to your specific setup.

### Step 2: Prepare the Test Data for Evaluation

To evaluate the model, you need to prepare the test data in a format that can be fed into the trained model for prediction. This typically involves converting the text data into numerical representations using the same preprocessing and feature extraction techniques applied during training.

Assuming you have already preprocessed the test data during the text preprocessing and feature extraction phase, you can proceed with vectorizing the preprocessed text using techniques such as Bag-of-Words or TF-IDF. Here, we will use the TF-IDF vectorization approach as an example.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Vectorize the preprocessed test data
X_test = vectorizer.transform(test_data['message'])

# Get the corresponding sentiment labels
y_test = test_data['sentiment']
```

In the above code snippet, we use the `TfidfVectorizer` from the `sklearn.feature_extraction.text` module to transform the preprocessed test data into TF-IDF vectors. The vectorized data is stored in the variable `X_test`, and the corresponding sentiment labels are stored in `y_test`.

### Step 3: Make Predictions on Test Data

Next, you will use the trained sentiment analysis model to make predictions on the test data. Depending on the algorithm you selected and trained in the previous steps, the prediction process may vary. Here, we assume you have a trained model named `model` ready for making predictions.

```python
# Make predictions on the test data
y_pred = model.predict(X_test)
```

The above code snippet uses the `predict()` method of the trained model to make predictions on the vectorized test data `X_test`. The predicted sentiment labels are stored in the variable `y_pred`.

### Step 4: Calculate Evaluation Metrics

Once you have the predicted labels (`y_pred`) and the true labels (`y_test`), you can calculate various evaluation metrics to assess the performance of your sentiment analysis model. Commonly used metrics for sentiment analysis include accuracy, precision, recall, and F1-score.

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Calculate precision
precision = precision

_score(y_test, y_pred, average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred, average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred, average='weighted')

# Print the evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
```

In the above code snippet, we use functions from the `sklearn.metrics` module to calculate the evaluation metrics. The `accuracy_score()` function computes the accuracy, while `precision_score()`, `recall_score()`, and `f1_score()` calculate precision, recall, and F1-score, respectively. Set the `average` parameter to `'weighted'` to account for class imbalance.

### Step 5: Interpret the Evaluation Results

After calculating the evaluation metrics, it's important to interpret the results to understand the performance of your sentiment analysis model. Here are some key points to consider:

- **Accuracy**: Measures the overall correctness of the predictions. A higher accuracy indicates better performance, but it may not be sufficient if the dataset is imbalanced.

- **Precision**: Indicates the proportion of correctly predicted positive sentiment instances out of all instances predicted as positive. Higher precision means fewer false positive predictions.

- **Recall**: Represents the proportion of correctly predicted positive sentiment instances out of all actual positive instances. Higher recall means fewer false negative predictions.

- **F1-score**: Harmonic mean of precision and recall. It provides a balanced measure of both metrics. Higher F1-score indicates a better balance between precision and recall.

Remember that the choice of evaluation metrics depends on the specific requirements of your sentiment analysis task and the importance of different types of errors (false positives vs. false negatives).

