# Matrix Factorization

In this module, we will explore the implementation of a matrix factorization technique for building a recommendation system. Specifically, we will focus on two popular methods: Singular Value Decomposition (SVD) and Alternating Least Squares (ALS). Matrix factorization techniques aim to decompose a user-item interaction matrix into lower-rank matrices, enabling the generation of recommendations based on latent factors.

## Step 1: Understanding Matrix Factorization

Matrix factorization is a collaborative filtering technique that models the user-item interactions as a matrix and decomposes it into two lower-rank matrices. These lower-rank matrices represent the latent factors that capture the underlying patterns in the data. The idea is to estimate missing entries or predict the ratings for items that users have not yet interacted with based on the learned latent factors.

### Example:

Consider the following user-item interaction matrix:

|         | Item 1 | Item 2 | Item 3 | Item 4 |
|---------|--------|--------|--------|--------|
| User 1  | 5      | 3      | 0      | 1      |
| User 2  | 4      | 0      | 0      | 1      |
| User 3  | 1      | 1      | 0      | 5      |
| User 4  | 1      | 0      | 0      | 4      |
| User 5  | 0      | 1      | 5      | 4      |

By applying matrix factorization, we aim to approximate this matrix as the product of two lower-rank matrices:

```
User-Item Interaction Matrix ≈ User Matrix x Item Matrix
```

## Step 2: Singular Value Decomposition (SVD)

Singular Value Decomposition (SVD) is a matrix factorization method that decomposes a matrix into three component matrices: U, Σ, and V^T.

- U represents the left singular vectors, which capture the latent factors related to users.
- Σ is a diagonal matrix containing the singular values, which indicate the importance of each latent factor.
- V^T represents the right singular vectors, which capture the latent factors related to items.

SVD can be used to predict missing ratings by multiplying the user and item matrices together.

### Example:

Consider the user-item interaction matrix from the previous example. We can perform SVD on this matrix to decompose it into three matrices:

```python
import numpy as np

# User-item interaction matrix
matrix = np.array([[5, 3, 0, 1],
                   [4, 0, 0, 1],
                   [1, 1, 0, 5],
                   [1, 0, 0, 4],
                   [0, 1, 5, 4]])

# Perform Singular Value Decomposition
U, sigma, Vt = np.linalg.svd(matrix, full_matrices=False)
```

After performing SVD, we obtain the following matrices:

- U matrix:

```python
U = np.array([[-0.577, 0.371, -0.496, -0.538],
              [-0.206, -0.865, -0.06, 0.44],
              [-0.13, 0.113, 0.823, -0.53],
              [-0.371, 0.324, -0.261, 0.796],
              [-0.66, -0.26, -0.133, -0.167]])
```

- Singular values (Σ):

```python
sigma = np.array([

9.43, 5.29, 1.84, 0.77])
```

- V^T matrix:

```python
Vt = np.array([[-0.251, -0.67, -0.404, -0.57],
               [0.505, -0.235, -0.228, -0.805],
               [-0.412, -0.455, 0.767, -0.159],
               [0.717, -0.508, -0.409, 0.257]])
```

These matrices represent the latent factors associated with users and items, which can be used for making recommendations.

## Step 3: Alternating Least Squares (ALS)

Alternating Least Squares (ALS) is another matrix factorization method that iteratively alternates between updating the user and item matrices to minimize the reconstruction error. ALS is particularly useful when dealing with sparse datasets.

ALS follows these steps:

1. Initialize the user and item matrices.
2. Repeat until convergence or a maximum number of iterations:
   - Update the user matrix while keeping the item matrix fixed.
   - Update the item matrix while keeping the user matrix fixed.
3. Use the learned matrices to make recommendations.

### Example:

Consider the same user-item interaction matrix used in the previous examples. We can apply ALS using a collaborative filtering library like Surprise in Python.

```python
from surprise import SVD
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import train_test_split

# Load the user-item interaction data
data = Dataset.load_builtin('ml-100k')

# Split the data into train and test sets
trainset, testset = train_test_split(data, test_size=0.2)

# Create an SVD model
model = SVD()

# Train the model on the training set
model.fit(trainset)

# Make predictions on the test set
predictions = model.test(testset)

# Evaluate the model using RMSE
accuracy.rmse(predictions)
```

By using the Surprise library, we can easily apply ALS for matrix factorization and evaluate the model's performance.

## Step 4: Comparing Performance

After implementing both SVD and ALS, it is crucial to compare their performance with the collaborative filtering and content-based filtering approaches. Evaluating the models using appropriate metrics, such as Root Mean Squared Error (RMSE) or Mean Average Precision (MAP), will help determine which technique provides the best recommendations for the given dataset.

It is important to note that the performance of matrix factorization techniques heavily depends on factors like dataset size, sparsity, and the specific characteristics of the user-item interactions.

