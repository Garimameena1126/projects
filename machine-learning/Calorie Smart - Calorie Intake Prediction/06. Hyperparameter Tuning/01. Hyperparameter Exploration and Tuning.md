

## Module 6: Hyperparameter Tuning (Optional)

In this module, we will explore different hyperparameter settings for the chosen regression model to improve its performance. Hyperparameters are parameters that are not learned from the data but set manually before training the model. Tuning these hyperparameters can significantly impact the model's performance and generalization ability.

### Step 1: Understand Hyperparameters

Before we start exploring different hyperparameter settings, let's understand the concept of hyperparameters and their role in machine learning models.

Hyperparameters are variables that determine how a machine learning model is trained and the characteristics of the resulting model. Unlike the parameters of a model, which are learned from the data during training, hyperparameters are set manually by the developer or researcher.

Common examples of hyperparameters include learning rate, number of hidden layers in a neural network, regularization strength, and batch size. These values can significantly affect the model's performance and should be carefully tuned to find the optimal combination.

### Step 2: Select Hyperparameters to Tune

The first step in hyperparameter tuning is to identify the hyperparameters that are most likely to have a significant impact on the model's performance. This selection is specific to the chosen regression model.

Let's assume we are using a neural network for calorie intake prediction. In this case, some hyperparameters that we might consider tuning are:

1. Learning rate: Determines the step size at each iteration during training.
2. Number of hidden layers: Defines the depth of the neural network.
3. Number of neurons in each hidden layer: Affects the model's capacity to learn complex patterns.
4. Regularization strength: Controls the trade-off between fitting the training data and preventing overfitting.
5. Activation function: Specifies the non-linear function applied to the outputs of each neuron.

### Step 3: Define a Hyperparameter Search Space

Once we have identified the hyperparameters to tune, we need to define a search space for each hyperparameter. The search space is a range or a set of possible values for each hyperparameter.

For example, the search space for the learning rate could be defined as `[0.001, 0.01, 0.1]`, and the search space for the number of hidden layers could be `[1, 2, 3]`. It's important to choose a reasonable range that covers a wide spectrum of values without being too exhaustive.

### Step 4: Choose a Hyperparameter Optimization Technique

To explore the defined search space and find the optimal combination of hyperparameters, we need to choose a hyperparameter optimization technique. Two common techniques are grid search and random search.

#### Grid Search

Grid search exhaustively searches through all possible combinations of hyperparameters in the defined search space. It trains and evaluates a model for each combination and returns the combination with the best performance.

Let's see an example of grid search using scikit-learn:

```python
from sklearn.model_selection import GridSearchCV

# Define the search space
param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'hidden_layers': [1, 2, 3],
    'neurons_per_layer': [32, 64, 128]
}

# Create the model to tune
model = NeuralNetwork()

# Perform grid search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
```

In this example, we define a search space for the

 learning rate, number of hidden layers, and number of neurons per layer. We then create an instance of the model to tune and perform grid search using `GridSearchCV` from scikit-learn. Finally, we retrieve the best hyperparameters using `best_params_`.

#### Random Search

Random search randomly samples hyperparameters from the defined search space. It performs a specified number of iterations and returns the combination with the best performance.

Here's an example of random search using scikit-learn:

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Define the search space
param_dist = {
    'learning_rate': uniform(0.001, 0.1),
    'hidden_layers': randint(1, 4),
    'neurons_per_layer': randint(32, 129)
}

# Create the model to tune
model = NeuralNetwork()

# Perform random search
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='neg_mean_squared_error', cv=5)
random_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
```

In this example, we define a search space using probability distributions from the `scipy.stats` module. We then create an instance of the model to tune and perform random search using `RandomizedSearchCV` from scikit-learn. Finally, we retrieve the best hyperparameters using `best_params_`.

### Step 5: Evaluate Model Performance with Best Hyperparameters

Once we have obtained the best hyperparameters from the optimization technique, it's important to evaluate the model's performance using these hyperparameters on an independent test set.

```python
# Create a new model with the best hyperparameters
best_model = NeuralNetwork(learning_rate=0.01, hidden_layers=2, neurons_per_layer=64)

# Train the best model
best_model.fit(X_train, y_train)

# Evaluate the best model on the test set
predictions = best_model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
```

In this example, we create a new instance of the model with the best hyperparameters and train it on the training set. We then use the trained model to make predictions on the test set and evaluate its performance using a suitable metric such as mean squared error (MSE).

