# Hyperparameter Tuning using Grid Search and Random Search

In this guide, we will explore the process of hyperparameter tuning for improving the performance of our chosen regression model. Specifically, we will use techniques like grid search and random search to find the optimal combination of hyperparameters. Hyperparameter tuning helps us fine-tune the model's settings to achieve better accuracy and generalization.

## Step 1: Understand Hyperparameters

Before we dive into hyperparameter tuning, let's first understand what hyperparameters are. Hyperparameters are configuration settings that are external to the model itself and need to be set before training the model. These settings control various aspects of the learning process and affect the model's performance. Examples of hyperparameters include learning rate, regularization strength, number of hidden layers, and number of trees in a random forest.

## Step 2: Identify Hyperparameters to Tune

The first step is to identify the hyperparameters of the chosen regression model that we want to tune. For example, if we are using the Gradient Boosting model, some hyperparameters we may want to tune are:
- `learning_rate`: controls the step size at each boosting iteration.
- `n_estimators`: the number of boosting stages (number of trees) to perform.
- `max_depth`: the maximum depth of the tree.
- `min_samples_split`: the minimum number of samples required to split an internal node.
- `min_samples_leaf`: the minimum number of samples required to be at a leaf node.

Note: The hyperparameters mentioned above are specific to the Gradient Boosting model. Different models may have different hyperparameters that can be tuned.

## Step 3: Prepare the Parameter Grid

For grid search and random search, we need to define a parameter grid, which is a dictionary containing different values for the hyperparameters we want to tune. The grid search algorithm will exhaustively search through all the combinations of hyperparameters to find the best combination.

Here's an example of a parameter grid for the Gradient Boosting model:

```python
param_grid = {
    'learning_rate': [0.1, 0.01, 0.001],
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 4, 6],
    'min_samples_leaf': [1, 2, 3]
}
```

In the above example, we have defined a range of values for each hyperparameter. Grid search will evaluate all possible combinations of these values.

## Step 4: Split the Data

To properly evaluate the performance of different hyperparameter settings, we need to split our preprocessed data into training and testing sets. The training set will be used for training the models, while the testing set will be used for evaluating their performance.

```python
from sklearn.model_selection import train_test_split

# X contains the features and y contains the target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## Step 5: Perform Grid Search

Now, let's perform grid search using the parameter grid and the training set. Grid search will train and evaluate models with all possible combinations of hyperparameters to find the best combination.

```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV

# Create an instance of the Gradient Boosting Regressor
model = GradientBoostingRegressor()

# Create the grid search object
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the grid search to the training data
grid

_search.fit(X_train, y_train)
```

In the above code, we create an instance of the Gradient Boosting Regressor and then create the grid search object. We pass the model, parameter grid, cross-validation value (cv), and scoring metric to the `GridSearchCV` constructor. Finally, we fit the grid search object to the training data.

## Step 6: Evaluate the Results

After grid search has completed, we can evaluate the results to determine the best combination of hyperparameters.

```python
# Print the best hyperparameters found by grid search
print("Best Hyperparameters: ", grid_search.best_params_)

# Evaluate the best model on the testing set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calculate evaluation metrics (e.g., RMSE)
from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Squared Error (RMSE): ", rmse)
```

In the above code, we print the best hyperparameters found by grid search using `grid_search.best_params_`. We then use the best model to make predictions on the testing set and calculate the root mean squared error (RMSE) to evaluate its performance.

## Step 7: Perform Random Search (Optional)

In addition to grid search, we can also perform random search to explore a wider range of hyperparameter combinations. Random search selects random combinations of hyperparameters and evaluates their performance.

```python
from sklearn.model_selection import RandomizedSearchCV

# Create the random search object
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)

# Fit the random search to the training data
random_search.fit(X_train, y_train)
```

In the above code, we create the random search object, pass the model, parameter grid, and other relevant parameters to the `RandomizedSearchCV` constructor, and fit it to the training data.

## Step 8: Evaluate the Results (Random Search)

After random search has completed, we can evaluate the results to determine the best combination of hyperparameters.

```python
# Print the best hyperparameters found by random search
print("Best Hyperparameters: ", random_search.best_params_)

# Evaluate the best model on the testing set
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calculate evaluation metrics (e.g., RMSE)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Squared Error (RMSE): ", rmse)
```

In the above code, we print the best hyperparameters found by random search using `random_search.best_params_`. We then use the best model to make predictions on the testing set and calculate the root mean squared error (RMSE) to evaluate its performance.

.