# Module 1: Data Collection and Preparation
## Task: Clean the data

Cleaning the data is an essential step in preparing the dataset for analysis and model training. In this task, we will handle missing values, remove duplicates, and address outliers if necessary. By performing these data cleaning operations, we can ensure the dataset's quality and integrity, which is crucial for accurate analysis and modeling.

Follow the step-by-step guide below to clean the data effectively.

### Step 1: Import Required Libraries

Before we begin, let's import the necessary libraries that we will be using throughout the data cleaning process.

```python
import pandas as pd
import numpy as np
```

### Step 2: Load the Data

Load the historical customer data into a Pandas DataFrame. Ensure that the data is in a format that can be easily read by Pandas, such as CSV or Excel.

```python
data = pd.read_csv('customer_data.csv')
```

### Step 3: Handling Missing Values

Missing values in the dataset can adversely affect the quality of our analysis and model training. Let's identify and handle missing values in the customer data.

#### Identify Missing Values

Check for missing values in the dataset using the `isnull()` function, which returns a DataFrame of the same shape as the input DataFrame but with boolean values indicating whether each element is missing or not.

```python
missing_values = data.isnull()
```

To get an overview of the missing values count in each column, you can sum the boolean values obtained above.

```python
missing_values_count = missing_values.sum()
```

#### Handle Missing Values

There are different strategies to handle missing values, depending on the nature and significance of the missing data. Here, we will discuss two common approaches: dropping missing values and imputing missing values.

##### Dropping Missing Values

If the missing values are relatively few and the corresponding rows or columns can be safely removed without losing critical information, we can drop those rows or columns.

To drop rows with missing values, use the `dropna()` function:

```python
data_without_missing_values = data.dropna(axis=0)
```

To drop columns with missing values, set the `axis` parameter to 1:

```python
data_without_missing_values = data.dropna(axis=1)
```

##### Imputing Missing Values

Imputing missing values involves filling in the missing data with estimated values. This approach can be useful when dropping missing values would result in a significant loss of information.

One simple imputation method is to replace missing values with the mean or median of the corresponding column.

```python
mean_imputed_data = data.fillna(data.mean())
median_imputed_data = data.fillna(data.median())
```

Alternatively, you can also forward-fill or backward-fill missing values using the `fillna()` function with the `method` parameter set to 'ffill' or 'bfill', respectively.

```python
forward_filled_data = data.fillna(method='ffill')
backward_filled_data = data.fillna(method='bfill')
```

### Step 4: Removing Duplicates

Duplicate records can skew the analysis and model training process. Let's identify and remove any duplicate rows from the dataset.

#### Identify Duplicate Rows

Use the `duplicated()` function to identify duplicate rows in the dataset. The function returns a boolean series indicating whether each row is a duplicate or not.

```python
duplicate_rows = data.duplicated()
```

#### Remove Duplicate Rows

To remove the duplicate rows from the dataset, use the `drop_duplicates()` function:

```python
data_without_duplicates = data.drop_duplicates()
```

### Step 5: Addressing Outliers

Outliers are extreme values that deviate significantly from the other data points in a dataset.

 Outliers can impact the statistical properties of the data and the performance of machine learning models. Let's explore how to address outliers.

#### Identify Outliers

A common method for identifying outliers is by calculating the z-scores for each data point. Z-scores measure how many standard deviations a data point is away from the mean. Typically, a z-score threshold of 3 or -3 is used to identify outliers.

```python
z_scores = (data - data.mean()) / data.std()
outliers = np.abs(z_scores) > 3
```

#### Address Outliers

There are several techniques for addressing outliers, such as capping, winsorization, or transforming the data. Here, we will discuss winsorization, which involves replacing extreme values with values at a certain percentile.

```python
winsorized_data = data.copy()
winsorized_data[outliers] = np.nanpercentile(data, q=99.9, axis=0)
```

### Step 6: Data Summary

After performing the data cleaning operations, it's essential to generate a summary of the cleaned dataset to verify that missing values, duplicates, and outliers have been appropriately handled.

```python
cleaned_data_summary = pd.DataFrame({
    'Original Data': [data.shape, missing_values_count.sum(), duplicate_rows.sum(), outliers.sum()],
    'Cleaned Data': [data_without_missing_values.shape, 0, data_without_duplicates.shape, 0]
}, index=['Shape', 'Missing Values', 'Duplicate Rows', 'Outliers'])
```

The `cleaned_data_summary` DataFrame provides a summary comparison between the original data and the cleaned data, including the shape of the data, the number of missing values, duplicate rows, and outliers.

