# Hyperparameter Tuning using Grid Search and Random Search

Hyperparameter tuning plays a crucial role in improving the performance of machine learning models. In this guide, we will explore two common techniques, namely Grid Search and Random Search, to find the optimal combination of hyperparameters for our chosen classification model. Let's get started!

## Step 1: Import the necessary libraries

Before we dive into hyperparameter tuning, let's import the required libraries and load our preprocessed dataset. We'll assume that you have already completed the data preprocessing step as mentioned in Module 2.

```python
import pandas as pd
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
```

## Step 2: Load and Split the Data

Let's load the preprocessed dataset and split it into training and testing sets. We'll use the training set for hyperparameter tuning and the testing set for evaluating the performance of the model with the tuned hyperparameters.

```python
# Load the preprocessed dataset (replace 'path_to_dataset' with the actual path)
dataset = pd.read_csv('path_to_dataset')

# Split the dataset into features (X) and labels (y)
X = dataset.drop('sleep_quality', axis=1)
y = dataset['sleep_quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## Step 3: Define the Model and Hyperparameter Space

Next, we need to define the classification model we'll be using and the hyperparameter space to explore. For this example, let's consider the Random Forest classifier and tune its `n_estimators` (number of trees) and `max_depth` (maximum depth of each tree) hyperparameters.

```python
# Define the classification model
model = RandomForestClassifier(random_state=42)

# Define the hyperparameter space
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10]
}
```

## Step 4: Perform Grid Search

Grid Search exhaustively searches through all the possible combinations of hyperparameters in the defined space and evaluates the model's performance using cross-validation. It helps us find the optimal hyperparameters that yield the best performance.

```python
# Perform Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)
```

## Step 5: Perform Random Search

Random Search randomly selects hyperparameter combinations from the defined space and evaluates the model's performance using cross-validation. It is a more efficient approach when the hyperparameter space is large.

```python
# Perform Random Search
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=5)
random_search.fit(X_train, y_train)

# Print the best hyperparameters and corresponding score
print("Best Hyperparameters: ", random_search.best_params_)
print("Best Score: ", random_search.best_score_)
```

## Step 6: Evaluate the Model

After performing hyperparameter tuning, let's evaluate the model's performance using the best hyperparameters obtained from either Grid Search or Random Search.

```python
# Evaluate the model on the testing set
y_pred = random_search.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on

 Testing Set: ", accuracy)
```

