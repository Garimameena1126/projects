# Module 3: Model Training and Evaluation

## Task: Fine-tune the Model's Hyperparameters to Improve Performance

In this task, we will focus on fine-tuning the hyperparameters of the sentiment analysis model to improve its performance. Hyperparameters are parameters that are not learned by the model during training but are set before training begins. They control the learning process and can significantly impact the model's performance.

To fine-tune the model's hyperparameters, we will follow the steps below:

### Step 1: Choose Hyperparameters to Tune

Before we begin, it's essential to identify the hyperparameters that can be tuned in the chosen classification algorithm. The hyperparameters vary depending on the algorithm selected. Let's assume we are using the Random Forest algorithm as an example.

For the Random Forest algorithm, some of the commonly tuned hyperparameters are:
- `n_estimators`: The number of decision trees in the random forest.
- `max_depth`: The maximum depth allowed for each decision tree.
- `min_samples_split`: The minimum number of samples required to split an internal node.
- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.
- `max_features`: The number of features to consider when looking for the best split.

These hyperparameters are just examples, and you may have additional or different hyperparameters depending on the algorithm you choose.

### Step 2: Define a Parameter Grid

To systematically search for the optimal combination of hyperparameters, we need to define a parameter grid. The parameter grid specifies the values to be explored for each hyperparameter. For example, we can define a parameter grid as follows:

```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}
```

In this example, we define different values to explore for each hyperparameter. You can adjust the values based on your specific needs.

### Step 3: Perform Grid Search Cross-Validation

Grid search cross-validation allows us to evaluate the model's performance across different hyperparameter combinations. It exhaustively searches through the parameter grid, trains and evaluates the model for each combination, and selects the hyperparameters that yield the best performance.

In Python, scikit-learn provides the `GridSearchCV` class, which automates this process. Here's an example of how to perform grid search cross-validation using the Random Forest algorithm:

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Assuming X_train and y_train are the preprocessed training data and labels, respectively

# Create a Random Forest classifier
rf = RandomForestClassifier()

# Create the GridSearchCV object with the classifier and parameter grid
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')

# Perform the grid search
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and the corresponding model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
```

In this example, we create a `RandomForestClassifier` object and initialize the `GridSearchCV` object with the classifier, the parameter grid, the number of cross-validation folds (`cv`), and the scoring metric (`scoring`). We then fit the grid search object to our preprocessed training data (`X_train` and `y_train`).

After the grid search completes, we can obtain the best hyperparameters using `grid_search.best_params_` and the corresponding best

 model using `grid_search.best_estimator_`.

### Step 4: Evaluate the Best Model

Now that we have the best model obtained from the grid search, we can evaluate its performance on the testing dataset. This step helps us assess how well the model generalizes to unseen data.

```python
# Assuming X_test and y_test are the preprocessed testing data and labels, respectively

# Evaluate the best model on the testing data
y_pred = best_model.predict(X_test)

# Calculate evaluation metrics (accuracy, precision, recall, F1-score)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
```

In this example, we use the `predict` method of the best model to generate predictions (`y_pred`) on the testing data (`X_test`). We then calculate the evaluation metrics, including accuracy, precision, recall, and F1-score using scikit-learn's `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` functions, respectively.

### Step 5: Iterate and Refine

Based on the evaluation results, you can analyze the model's performance and iterate on the fine-tuning process. You may want to adjust the parameter grid, explore different hyperparameter combinations, or even try different algorithms.

Remember to avoid overfitting the hyperparameters to the testing dataset. If you find that the model's performance is not satisfactory on new, unseen data, consider revisiting the preprocessing steps or exploring other machine learning algorithms.

