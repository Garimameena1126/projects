# Feature Extraction using TF-IDF and Word Embeddings

In this guide, we will explore two common feature extraction methods for text data: TF-IDF and word embeddings. Feature extraction plays a crucial role in machine learning tasks, as it transforms raw text into numerical representations that can be used as input to machine learning algorithms. TF-IDF (Term Frequency-Inverse Document Frequency) is a traditional method that computes the importance of each word in a document, while word embeddings represent words as dense vector representations capturing semantic meaning. We will cover the implementation of TF-IDF and demonstrate the use of pre-trained word embeddings such as Word2Vec and GloVe.

## Table of Contents
1. [TF-IDF Feature Extraction](#tfidf)
   - Importing Required Libraries
   - Preprocessing the Text Data
   - Computing TF-IDF Features
2. [Word Embeddings](#word-embeddings)
   - Using Pre-trained Word Embeddings
     - Word2Vec
     - GloVe

Let's get started!

## 1. TF-IDF Feature Extraction <a name="tfidf"></a>

### Importing Required Libraries

Before we begin, let's import the necessary libraries for text preprocessing and TF-IDF feature extraction:

```python
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
```

Make sure you have NLTK (Natural Language Toolkit) installed by running `pip install nltk` in your terminal.

### Preprocessing the Text Data

To use TF-IDF, we first need to preprocess our text data. Preprocessing involves cleaning the text by removing unnecessary characters, converting text to lowercase, and removing stop words. We will use NLTK for stop word removal.

Here's an example of how to preprocess text data:

```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import re

nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    
    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    
    # Tokenize the text into words
    tokens = word_tokenize(text)
    
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    
    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    
    # Join the tokens back into a single string
    preprocessed_text = ' '.join(tokens)
    
    return preprocessed_text
```

### Computing TF-IDF Features

Now that we have preprocessed the text data, we can compute the TF-IDF features using the `TfidfVectorizer` class from scikit-learn.

Here's an example of how to compute TF-IDF features:

```python
def compute_tfidf_features(corpus):
    # Create an instance of TfidfVectorizer
    vectorizer = TfidfVectorizer()
    
    # Fit and transform the corpus to compute the TF-IDF features
    tfidf_features = vectorizer.fit_transform(corpus)
    
    return tfidf_features
```

To use these functions, provide a list of documents (corpus) as input. The `preprocess_text` function will preprocess each document, and the `compute_tfidf_features` function will return a sparse matrix representing the TF-IDF features for the corpus.

## 2. Word Embeddings <a name="word-embeddings"></a>

Word embeddings are dense vector representations that capture semantic meaning of words. They are trained on large text corpora and can be used to represent words as numerical vectors in machine learning models. We will explore two popular pre-trained word embedding models: Word2Vec and GloVe.

### Using Pre-trained Word Embeddings

#### Word2Vec

Word2Vec is a popular word embedding model that learns word representations by predicting the context of words in a large corpus. We can use pre-trained Word2Vec models to obtain word embeddings for our text data.

To use Word2Vec, you need to download the pre-trained model file. You can find pre-trained Word2Vec models on the internet or use the `gensim` library to load them.

Here's an example of how to load a pre-trained Word2Vec model and obtain word embeddings:

```python
import gensim

# Load the pre-trained Word2Vec model
word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('path_to_pretrained_model.bin', binary=True)

def get_word_embeddings(text):
    # Tokenize the text into words
    tokens = word_tokenize(text)
    
    # Initialize an empty list to store word embeddings
    embeddings = []
    
    # Iterate over each word in the text
    for word in tokens:
        try:
            # Get the word embedding vector
            embedding = word2vec_model[word]
            embeddings.append(embedding)
        except KeyError:
            # Handle out-of-vocabulary words
            continue
    
    return embeddings
```

Replace `'path_to_pretrained_model.bin'` with the actual path to your pre-trained Word2Vec model file. The `get_word_embeddings` function takes a text as input and returns a list of word embeddings.

#### GloVe

GloVe (Global Vectors for Word Representation) is another widely used word embedding model that learns word vectors based on global co-occurrence statistics. Similar to Word2Vec, we can use pre-trained GloVe models to obtain word embeddings.

To use GloVe, you need to download the pre-trained model file. You can find pre-trained GloVe models on the internet or use the `gensim` library to load them.

Here's an example of how to load a pre-trained GloVe model and obtain word embeddings:

```python
import gensim

# Load the pre-trained GloVe model
glove_model = gensim.models.KeyedVectors.load_word2vec_format('path_to_pretrained_model.txt', binary=False)

def get_word_embeddings(text):
    # Tokenize the text into words
    tokens = word_tokenize(text)
    
    # Initialize an empty list to store word embeddings
    embeddings = []
    
    # Iterate over each word in the text
    for word in tokens:
        try:
            # Get the word embedding vector
            embedding = glove_model[word]
            embeddings.append(embedding)
        except KeyError:
            # Handle out-of-vocabulary words
            continue
    
    return embeddings
```

Replace `'path_to_pretrained_model.txt'` with the actual path to your pre-trained GloVe model file. The `get_word_embeddings` function takes a text as input and returns a list of word embeddings.

