# Perform Feature Engineering for Destination Recommendation System

In this guide, we will focus on the task of performing feature engineering for the Destination Recommendation System. Feature engineering involves creating new features or transforming existing ones to capture important aspects of the destinations. We will cover techniques to handle missing values, encode categorical variables, normalize numerical features, and generate new features based on the existing data.

Let's proceed with the step-by-step process of feature engineering:

## Step 1: Import the Required Libraries
Before we begin, let's import the necessary libraries for data preprocessing and feature engineering. We will be using the following libraries in this guide:
- pandas: for data manipulation and analysis
- scikit-learn: for preprocessing tasks

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
```

## Step 2: Load the Dataset
Load the dataset that contains information about travel destinations. This dataset should already be collected and explored in the previous module (Module 1: Data Collection and Exploration). Assuming the dataset is in CSV format, we can use the `pd.read_csv()` function to load it into a pandas DataFrame.

```python
# Replace 'dataset.csv' with the actual filename and path of your dataset
dataset = pd.read_csv('dataset.csv')
```

## Step 3: Handling Missing Values
Missing values are common in datasets and can adversely affect the performance of machine learning models. We need to handle missing values appropriately before performing feature engineering. There are several strategies to handle missing values, such as:
- Removing rows or columns with missing values
- Imputing missing values with statistical measures (mean, median, mode)
- Using advanced imputation techniques (regression models, nearest neighbors)

Let's demonstrate how to impute missing values with the mean of the respective columns using the `fillna()` method from pandas.

```python
# Impute missing values with the mean
dataset.fillna(dataset.mean(), inplace=True)
```

## Step 4: Encoding Categorical Variables
Machine learning models typically require numerical inputs, so we need to encode categorical variables into numeric representations. One common encoding technique is Label Encoding, which assigns a unique integer value to each category.

```python
# Encode categorical variables using LabelEncoder
label_encoder = LabelEncoder()

dataset['category_encoded'] = label_encoder.fit_transform(dataset['category'])
```

## Step 5: Normalizing Numerical Features
To ensure that all numerical features are on a similar scale, it is recommended to normalize or standardize them. Here, we will demonstrate how to perform Min-Max scaling, which scales the values to a specific range (e.g., 0 to 1).

```python
# Perform Min-Max scaling on numerical features
scaler = MinMaxScaler()

dataset['numerical_feature_scaled'] = scaler.fit_transform(dataset[['numerical_feature']])
```

## Step 6: Generating New Features
Feature engineering often involves creating new features that capture important aspects of the data. These features can be derived from existing variables or through domain knowledge. Let's demonstrate how to create a new feature based on existing ones.

```python
# Create a new feature by combining existing features
dataset['new_feature'] = dataset['feature1'] + dataset['feature2']
```

## Step 7: Verify the Transformed Dataset
After performing feature engineering, it's essential to examine the transformed dataset to ensure that the changes have been applied correctly. Verify the new features, encoded variables, and scaled features to confirm the desired transformations.

```python
# Display the updated dataset
print(dataset.head())
```

